{
    "title": "MTG Card Sorting Roboter",
    "chapters":
        [
        {            
            "paragraphs": [
                {"type": "html", "tag":"Introduction", "src":"<p>In recent years, Transformers, originally built for text, have offered a powerful alternative to Convolutional Neural Networks in Computer Vision. However, like for any Deep Neural Network, it is challenging to achieve an insight into their decision making process. In this educational blog post, we interactively interpret Vision Transformer classifying images by using explainability methods. We divide the explainability methods into two sections:<b> Model Externals</b>, where we examine the relationship between the input (image) and the output (class prediction) of the model, and <b>Model Internals</b>, where we mechanistically interpret single neurons and attention heads.<br>Past works [<a href= https://jacobgil.github.io/deeplearning/vision-transformer-explainability target= _blank>1</a>, <a href= https://arxiv.org/pdf/2311.06786 target= _blank>2</a>, <a href= https://www.mdpi.com/2079-9292/13/1/175 target= _blank>3</a>] have already explored Vision Transformers by using explainability methods. Nevertheless, their lack of interactivity limits their power to effectively provide an understanding overview to the reader. Furthermore, a scarce choice of explainability methods misses out on the complementary information that can be gained from applying and comparing various methods. This is crucial for providing a transparent and accurate insight into Vision Transformers to the reader. In this blogpost, we close this gap by: <ol><li>Interactively exploring the Vision Transformer’s architecture and explainability methods applied to it.<ol><li>Helping the user to identify the crucial information from input images used for the classification task.</li><li>Analyzing what excites single neurons at various network depths to investigate what the most modular parts of the network have learned in various layers.</li><li><b>Introducing a new way to adapt the activation maximization method to attention head’s attention scores.</b> To the best of our knowledge, we are the first to introduce this method. It allows to trace the change of what each attention head attends to as the network depths increases and thus provides a first step to better understand the task of individual attention heads.</li></ol><li>Providing transparency on the limitations of each method by using an occlusion-based interaction to test the information provided by the different explainability methods.</li></ol>A <b>key finding</b> of our blogpost is that our Vision Transformer model generalizes very well, taking into account a various set of features of the input image that belong to the predicted object and the surrounding environment, also called context. We furthermore find that the focus of each neuron and attention head shifts to increasingly complex information and patterns at deeper layers.<p>"}            
            ]    
        }
    ]
}