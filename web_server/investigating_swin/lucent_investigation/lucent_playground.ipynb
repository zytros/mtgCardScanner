{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224').eval()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "obj = f\"vit_encoder_layer_0_layernorm_after:767\" # layer_0-11\n",
    "# render.render_vis(model, obj, lambda: param.image(224, 224, fft=False, channels=3), show_inline=True, transforms=lambda x:x)\n",
    "img = render.render_vis(model, obj, save_image = True, show_inline=True, image_name = \"test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Module\", model.__module__)\n",
    "# print(get_model_layers(model))\n",
    "\n",
    "# inputs = processor(images=image, return_tensors=\"pt\")\n",
    "# print(inputs[\"pixel_values\"].shape)\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "\n",
    "for layer in range(1, 12):\n",
    "    for neuron in range(0, 768, 50):\n",
    "            obj = f\"vit_encoder_layer_{layer}_layernorm_after:{neuron}\" # layer_0-11\n",
    "            image_name = f\"vit_encoder_layer_{layer}_layernorm_after_{neuron}.jpg\"\n",
    "            # render.render_vis(model, obj, lambda: param.image(224, 224, fft=False, channels=3), show_inline=True, transforms=lambda x:x)\n",
    "            img = render.render_vis(model, obj, show_inline=True, save_image=True, image_name = image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indian elephant: 385\n",
    "# ⁠persian cat: 283\n",
    "# ⁠Goose: 99\n",
    "# ⁠Model T: 661\n",
    "# ⁠Harp: 594\n",
    "    \n",
    "class_indices = [385, 283, 99, 661, 594]\n",
    "class_names = [\"IndianElephant\", \"PersianCat\", \"Goose\", \"ModelT\", \"Harp\"]\n",
    "for list_idx, neuron_class in enumerate(class_indices):\n",
    "    obj = f\"classifier:{str(neuron_class)}\"\n",
    "    image_name = f\"classifier_{class_names[list_idx]}.jpg\"\n",
    "    img = render.render_vis(model, obj, show_inline=True, save_image=True, image_name = image_name, thresholds=[2560])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AM for a complete ViT Layer\n",
    "for layer in range(0, 12):\n",
    "    obj = f\"vit_encoder_layer_{layer}_layernorm_after\" # layer_0-11\n",
    "    image_name = f\"vit_encoder_layer_{layer}_layernorm_after.jpg\"\n",
    "    # render.render_vis(model, obj, lambda: param.image(224, 224, fft=False, channels=3), show_inline=True, transforms=lambda x:x)\n",
    "    img = render.render_vis(model, obj, show_inline=True, save_image=True, image_name = image_name, thresholds=[2560])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output maximization for attention heads\n",
    "# Note: The ViTClassifier seems to have a fundamental error in its self-attention layer, where they don't divide the input onto the heads.\n",
    "# Instead, they use the same dense layers (Q,K,V) for all attention heads, which is the same as if there was only one attention head.\n",
    "for layer in range(0, 12):\n",
    "    for head in range(0, 12):\n",
    "        obj = f\"vit_encoder_layer_{layer}_attention_attention_output_context:{head}\" # layer_0-11\n",
    "        image_name = f\"vit_encoder_layer_{layer}_attention_attention_output_context_{head}.jpg\"\n",
    "        # render.render_vis(model, obj, lambda: param.image(224, 224, fft=False, channels=3), show_inline=True, transforms=lambda x:x)\n",
    "        img = render.render_vis(model, obj, show_inline=True, save_image=True, image_name = image_name, thresholds=[560])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output maximization for attention heads in grid\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "fig,ax = plt.subplots(12,12, tight_layout=True, figsize=(20,20))\n",
    "\n",
    "filenames=[[f'AM_outputs/attention_heads_output_maximization/vit_encoder_layer_{i}_attention_attention_output_context_{j}.jpg' for j in range(12)] for i in range(12)] #or glob or any other way to describe filenames\n",
    "for transformer_block in range(12):\n",
    "    for head in range(12):\n",
    "        with open(filenames[transformer_block][head],'rb') as f:\n",
    "            image= Image.open(f)\n",
    "            ax[head][transformer_block].set_axis_off()\n",
    "            ax[head][transformer_block].imshow(image)\n",
    "            \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try activation grids by saving the activation of passing a specific image. Then we optimize the input to correspond to exactly this activation vector. We can do so attention-head and transformer-layer-wise. This kind of corresponds to how the network \"sees\" the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.36s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 60), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/IXML/lib/python3.8/site-packages/PIL/Image.py:3070\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3070\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 60), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 288\u001b[0m\n\u001b[1;32m    286\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit.encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit.encoder.layer[0].attention.attention.output_context\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_grid_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 170\u001b[0m, in \u001b[0;36mactivation_grid_vit\u001b[0;34m(img, model, layer, n_steps)\u001b[0m\n\u001b[1;32m    168\u001b[0m     grid \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(imgs, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(nb_cells)), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    169\u001b[0m     grid \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_head_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattention_head\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# grid = grid.numpy()\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Image.fromarray(grid).save(f\"attention_head_{attention_head}.png\")\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# render.export(grid, f\"attention_head_{attention_head}.png\")\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m# negative matrix factorization `NMF` is used to reduce the number\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03mrender.show(grid)\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mreturn imgs \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/IXML/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/IXML/lib/python3.8/site-packages/torchvision/utils.py:149\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[1;32m    148\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 149\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mndarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m im\u001b[38;5;241m.\u001b[39msave(fp, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/IXML/lib/python3.8/site-packages/PIL/Image.py:3073\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3072\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m typekey\n\u001b[0;32m-> 3073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 60), |u1"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "# from lucent.modelzoo.util import get_model_layers\n",
    "# from lucent.optvis import render, param, transform, objectives\n",
    "# import requests\n",
    "# import torch\n",
    "\n",
    "# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224').eval().to(device)\n",
    "# # print(model)\n",
    "\n",
    "# # Output maximization for attention heads\n",
    "# # Note: The ViTClassifier seems to have a fundamental error in its self-attention layer, where they don't divide the input onto the heads.\n",
    "# # Instead, they use the same dense layers (Q,K,V) for all attention heads, which is the same as if there was only one attention head.\n",
    "# # for layer in range(0, 12):\n",
    "# #     for head in range(0, 12):\n",
    "# #         obj = f\"vit_encoder_layer_{layer}_attention_attention_output_context:{head}\" # layer_0-11\n",
    "# #         image_name = f\"vit_encoder_layer_{layer}_attention_attention_output_context_{head}.jpg\"\n",
    "# #         # render.render_vis(model, obj, lambda: param.image(224, 224, fft=False, channels=3), show_inline=True, transforms=lambda x:x)\n",
    "# #         img = render.render_vis(model, obj, save_image=True, image_name = image_name, thresholds=[2560])\n",
    "\n",
    "# # indian elephant: 385\n",
    "# # ⁠persian cat: 283\n",
    "# # ⁠Goose: 99\n",
    "# # ⁠Model T: 661\n",
    "# # ⁠Harp: 594\n",
    "    \n",
    "# class_indices = [385, 283, 99, 661, 594]\n",
    "# class_names = [\"IndianElephant\", \"PersianCat\", \"Goose\", \"ModelT\", \"Harp\"]\n",
    "# for list_idx, neuron_class in enumerate(class_indices):\n",
    "#     obj = f\"classifier:{str(neuron_class)}\"\n",
    "#     image_name = f\"classifier_{class_names[list_idx]}.jpg\"\n",
    "#     img = render.render_vis(model, obj, show_inline=True, save_image=True, image_name = image_name, thresholds=[2560])\n",
    "\n",
    "# activation grids\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "from lucent.modelzoo import *\n",
    "from lucent.misc.io import show\n",
    "import lucent.optvis.objectives as objectives\n",
    "import lucent.optvis.param as param\n",
    "import lucent.optvis.render as render\n",
    "import lucent.optvis.transform as transform\n",
    "from lucent.misc.channel_reducer import ChannelReducer\n",
    "from lucent.misc.io import show\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_layer(model, layer, X):\n",
    "    layers = layer.split(\".\")\n",
    "    curr_layer = model\n",
    "    for layer in layers:\n",
    "        curr_layer = eval(\"curr_layer.\" + layer) # hackerman to allow recursion even through module lists\n",
    "    hook = render.ModuleHook(curr_layer)\n",
    "    model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, acts, patch_no, batch=1):\n",
    "    acts = torch.from_numpy(acts).to(device)\n",
    "    def inner(T):\n",
    "        pred = T(layer)\n",
    "        # print(pred.shape) # (196, 197, 768)\n",
    "        pred = pred[patch_no, patch_no+1, batch*64:(batch+1)*64] # get the 64 neurons corresponding to that attention head and that patch\n",
    "        # print(pred.shape) # (64)\n",
    "        # print(acts.shape) # (64)\n",
    "        return -(pred * acts).sum(dim=0, keepdims=True).mean()\n",
    "    return inner\n",
    "\n",
    "def activation_grid_vit(\n",
    "    img,\n",
    "    model,\n",
    "    layer,\n",
    "    n_steps=1024\n",
    "):\n",
    "    cell_image_size=16 # size of each patch\n",
    "    # Normalize and resize the image\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    normalize = (transform.normalize())\n",
    "    transforms = transform.standard_transforms.copy() + [\n",
    "        normalize,\n",
    "        torch.nn.Upsample(size=224, mode=\"bilinear\", align_corners=True),\n",
    "    ]\n",
    "    transforms_f = transform.compose(transforms)\n",
    "    # shape: (1, 3, original height of img, original width of img)\n",
    "    img = img.unsqueeze(0)\n",
    "    # shape: (1, 3, 224, 224)\n",
    "    img = transforms_f(img)\n",
    "\n",
    "\n",
    "    attention_scores = get_layer(model, layer, img)[0]\n",
    "    # print(attention_scores.shape) # (197, 768)\n",
    "    attention_scores = attention_scores.reshape([197, 12, 64])\n",
    "    \n",
    "    attention_scores_np = attention_scores.cpu().numpy()\n",
    "    # print(attention_scores_np.shape) # (197, 12, 64)\n",
    "    num_patches, n_attention_heads, _ = attention_scores_np.shape\n",
    "    num_patches -= 1 # remove the x_class\n",
    "    \n",
    "    # do naive implementation first\n",
    "    \n",
    "    # for each position `(y, x)` in the feature map `acts`, we optimize an image\n",
    "    # to match with the features `acts[:, y, x]`\n",
    "    # This means that the total number of cells (which is the batch size here) \n",
    "    # in the grid is layer_height*layer_width.\n",
    "    nb_cells = (num_patches-1) * n_attention_heads # cell for each patch for each activation head\n",
    "    \n",
    "    # rename layer so that the render function finds it\n",
    "    layer_converted = layer.replace(\".\", \"_\")\n",
    "    layer_converted = layer_converted.replace(\"[\", \"_\")\n",
    "    layer_converted = layer_converted.replace(\"]\", \"\")\n",
    "    \n",
    "    for attention_head in range(n_attention_heads):\n",
    "        # Parametrization of the of each cell in the grid\n",
    "        param_f = lambda: param.image(\n",
    "            h=cell_image_size, w=cell_image_size, batch=num_patches\n",
    "        )\n",
    "\n",
    "        obj = objectives.Objective.sum(\n",
    "            [\n",
    "                # for each position in `acts`, maximize the dot product between the activations\n",
    "                # `acts` at the position (y, x) and the features of the corresponding\n",
    "                # cell image on our 'grid'. The activations at (y, x) is a vector of size\n",
    "                # `layer_channels` (this depends on the `layer`). The features\n",
    "                # of the corresponding cell on our grid is a tensor of shape\n",
    "                # (layer_channels, cell_layer_height, cell_layer_width).\n",
    "                # Note that cell_layer_width != layer_width and cell_layer_height != layer_weight\n",
    "                # because the cell image size is smaller than the image size.\n",
    "                # With `dot_compare`, we maximize the dot product between\n",
    "                # cell_activations[y_cell, x_xcell] and acts[y,x] (both of size `layer_channels`)\n",
    "                # for each possible y_cell and x_cell, then take the average to get a single\n",
    "                # number. Check `dot_compare for more details.`\n",
    "                # dot_compare(layer, attention_scores_np[y+1:y+2, x:x+1], batch=x + y * 64)\n",
    "                # for i, (x, y) in enumerate(product(range(num_patches), range(n_attention_heads)))\n",
    "                dot_compare(layer_converted, attention_scores_np[x+1, attention_head], patch_no=x, batch=attention_head) # skip the first patch (x_class)\n",
    "                for i, x in enumerate(range(num_patches)) # try for one attention head first\n",
    "            ]\n",
    "        )\n",
    "        results = render.render_vis(\n",
    "            model,\n",
    "            obj,\n",
    "            param_f,\n",
    "            thresholds=(n_steps,),\n",
    "            progress=True,\n",
    "            fixed_image_size=224,\n",
    "            show_image=False,\n",
    "            save_image=True,\n",
    "            image_name=f\"attention_head_{attention_head}.png\"\n",
    "        )\n",
    "        # shape: (layer_height*layer_width, cell_image_size, cell_image_size, 3)\n",
    "        imgs = results[-1] # last step results\n",
    "        # shape: (layer_height*layer_width, 3, cell_image_size, cell_image_size)\n",
    "        imgs = imgs.transpose((0, 3, 1, 2))\n",
    "        imgs = torch.from_numpy(imgs)\n",
    "        imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "        # turn imgs into a a grid\n",
    "        grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(nb_cells)), padding=0)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        torchvision.utils.save_image(grid, f\"attention_head_{attention_head}.png\")\n",
    "        # grid = grid.numpy()\n",
    "        # Image.fromarray(grid).save(f\"attention_head_{attention_head}.png\")\n",
    "        # render.export(grid, f\"attention_head_{attention_head}.png\")\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # negative matrix factorization `NMF` is used to reduce the number\n",
    "    # of channels to n_groups. This will be used as the following.\n",
    "    # Each cell image in the grid is decomposed into a sum of\n",
    "    # (n_groups+1) images. First, each cell has its own set of parameters\n",
    "    #  this is what is called `cells_params` (see below). At the same time, we have\n",
    "    # a of group of images of size 'n_groups', which also have their own image parametrized\n",
    "    # by `groups_params`. The resulting image for a given cell in the grid\n",
    "    # is the sum of its own image (parametrized by `cells_params`)\n",
    "    # plus a weighted sum of the images of the group. Each image from the group\n",
    "    # is weighted by `groups[cell_index, group_idx]`. Basically, this is a way of having\n",
    "    # the possibility to make cells with similar activations have a similar image, because\n",
    "    # cells with similar activations will have a similar weighting for the elements\n",
    "    # of the group.\n",
    "    \n",
    "    # reducer = ChannelReducer(n_patches, \"NMF\")\n",
    "    \n",
    "    attention_scores_np /= attention_scores_np.max(0) # (197, 768)\n",
    "    \n",
    "    attention_scores = torch.from_numpy(attention_scores_np)\n",
    "\n",
    "    # Parametrization of the images of the groups (we have 12 activation groups / aka heads)\n",
    "    attention_heads_params, attention_heads_image_f = param.fft_image(\n",
    "        [64, 3, 16, 16] # every patch has 64 attention scores per patch\n",
    "    )\n",
    "    # Parametrization of the images of each patch in the grid (we have 196 patches + x_class)\n",
    "    patches_params, patches_images_f = param.fft_image(\n",
    "        [num_patches, 3, 16, 16] # every patch has channels RGB and size [16,16]\n",
    "    )\n",
    "\n",
    "    # First, we need to construct the images of the grid\n",
    "    # from the parameterizations\n",
    "\n",
    "    def image_f():\n",
    "        attention_heads = attention_heads_image_f()\n",
    "        patches_images = patches_images_f()\n",
    "        X = []\n",
    "        for i in range(num_patches):\n",
    "            x = 0.7 * patches_images[i] + 0.5 * sum(\n",
    "                attention_scores[i+1, j*64: (j+1)*64].squeeze() for j in range(n_attention_heads) # * attention_heads[j] # (i+1) to skip the x_class, i symbolizes the index of the patch; than take 64 attention scores per attention head.\n",
    "            )\n",
    "            X.append(x)\n",
    "        X = torch.stack(X)\n",
    "        return X\n",
    "\n",
    "    # make sure the images are between 0 and 1\n",
    "    image_f = param.to_valid_rgb(image_f, decorrelate=True)\n",
    "\n",
    "    # After constructing the cells images, we sample randomly a mini-batch of cells\n",
    "    # from the grid. This is to prevent memory overflow, especially if the grid\n",
    "    # is large.\n",
    "    # def sample(image_f, batch_size):\n",
    "    #     def f():\n",
    "    #         X = image_f()\n",
    "    #         inds = torch.randint(0, len(X), size=(batch_size,))\n",
    "    #         inputs = X[inds]\n",
    "    #         # HACK to store indices of the mini-batch, because we need them\n",
    "    #         # in objective func. Might be better ways to do that\n",
    "    #         sample.inds = inds\n",
    "    #         return inputs\n",
    "\n",
    "    #     return f\n",
    "\n",
    "    # image_f_sampled = sample(image_f, batch_size=batch_size)\n",
    "\n",
    "    # Now, we define the objective function\n",
    "\n",
    "    def objective_func(model):\n",
    "        # shape: (batch_size, layer_channels, cell_layer_height, cell_layer_width)\n",
    "        pred = model(layer)\n",
    "        # use the sampled indices from `sample` to get the corresponding targets\n",
    "        # target = attention_scores[sample.inds].to(pred.device)\n",
    "        target = attention_scores.to(pred.device)\n",
    "        \n",
    "        # shape: (batch_size, layer_channels, 1, 1)\n",
    "        target = target.view(target.shape[0], target.shape[1], 1, 1)\n",
    "        dot = (pred * target).sum(dim=1).mean()\n",
    "        return -dot\n",
    "\n",
    "    obj = objectives.Objective(objective_func)\n",
    "\n",
    "    def param_f():\n",
    "        # We optimize the parametrizations of both the groups and the cells\n",
    "        params = list(attention_heads_params) + list(patches_params)\n",
    "        return params, image_f\n",
    "        # return params, image_f_sampled\n",
    "\n",
    "    results = render.render_vis(\n",
    "        model,\n",
    "        obj,\n",
    "        param_f,\n",
    "        thresholds=(n_steps,),\n",
    "        show_image=False,\n",
    "        progress=True,\n",
    "        fixed_image_size=cell_image_size,\n",
    "    )\n",
    "    # shape: (layer_height*layer_width, 3, grid_image_size, grid_image_size)\n",
    "    imgs = image_f()\n",
    "    imgs = imgs.cpu().data\n",
    "    imgs = imgs[:, :, 2:-2, 2:-2]\n",
    "    # turn imgs into a a grid\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=int(np.sqrt(num_patches)), padding=0)\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    grid = grid.numpy()\n",
    "    render.show(grid)\n",
    "    return imgs \"\"\"\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224').eval().to(device)\n",
    "img = np.array(Image.open(\"cat.png\"), np.float32)\n",
    "layer = \"vit.encoder\"\n",
    "layer = \"vit.encoder.layer[0].attention.attention.output_context\"\n",
    "_ = activation_grid_vit(img, model, layer=layer, n_steps=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IXML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
